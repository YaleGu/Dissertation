{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import datetime\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.tsaplots import plot_acf  \n",
    "from statsmodels.tsa.stattools import adfuller as ADF  \n",
    "from statsmodels.graphics.tsaplots import plot_pacf    \n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox  \n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "!pip install pmdarima\n",
    "import pmdarima as pm\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,explained_variance_score,mean_absolute_error,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mobility data set\n",
    "\n",
    "url = 'Mobility_Report.csv'\n",
    "df= pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data about London\n",
    "\n",
    "London = df[df['sub_region_1']=='Greater London']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### London['sub_region_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SampleSet=London.drop(['retail_and_recreation_percent_change_from_baseline','parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline','workplaces_percent_change_from_baseline',\n",
    "                       'residential_percent_change_from_baseline'],axis=1)\n",
    "SampleSet.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert data type\n",
    "\n",
    "London['date'] = pd.to_datetime(London['date'],format='%d/%m/%Y')\n",
    "#London['date'] = London['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only mobility for groceries\n",
    "\n",
    "London1 = London[['sub_region_1','sub_region_2','date','grocery_and_pharmacy_percent_change_from_baseline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London1 = London1.pivot(index='sub_region_2', columns='date', values='grocery_and_pharmacy_percent_change_from_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London1.index = pd.Series(London1.index).replace(np.nan, 'London_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London1 = London1.T\n",
    "London1[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#London1.to_csv('London_mobility.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London1[:130].plot(figsize=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series of each borough\n",
    "\n",
    "fig, axs = plt.subplots(7,5,figsize=(30,30))\n",
    "fig.suptitle('Series')\n",
    "lims = [np.datetime64('2020-02-15'), np.datetime64('2021-05-28')]\n",
    "for i in range(7):\n",
    "    for j in range(5):\n",
    "        if i*5+j>len(London1.columns): # pass the others that we can't fill\n",
    "            continue\n",
    "        axs[i, j].set_title(London1.columns[i*5+j])\n",
    "        axs[i, j].plot(London1[London1.columns[i*5+j]].values)\n",
    "        for labels in axs[i,j].get_xticklabels():\n",
    "            labels.set_rotation(40)\n",
    "            labels.set_horizontalalignment('right')\n",
    "            \n",
    "plt.savefig('TimeSeries.png') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data for the study timeframe\n",
    "\n",
    "test = London1[:130]\n",
    "\n",
    "fig, axs = plt.subplots(7,5,figsize=(30,30))\n",
    "fig.suptitle('Series')\n",
    "\n",
    "for i in range(7):\n",
    "    for j in range(5):\n",
    "        if i*5+j>len(test.columns): # pass the others that we can't fill\n",
    "            continue\n",
    "        axs[i, j].set_title(test.columns[i*5+j])\n",
    "        axs[i, j].plot(test[test.columns[i*5+j]].values)\n",
    "        for labels in axs[i,j].get_xticklabels():\n",
    "            labels.set_rotation(40)\n",
    "            labels.set_horizontalalignment('right')\n",
    "#plt.savefig('TimeSeries2.pdf',dpi=1200) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fig5_1 = London1.drop('London_total',axis=1).T\n",
    "#Fig5_1.to_csv('Fig5_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig5_1 = pd.read_csv('Fig5_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig5_1.index = Fig5_1['sub_region_2']\n",
    "Fig5_1 = Fig5_1.drop('sub_region_2',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of time series\n",
    "\n",
    "heat_map = plt.figure(figsize=(30,20))\n",
    "sns_plot = sns.heatmap(Fig5_1,center=0, cmap=\"RdBu_r\",vmin=-100,vmax=100,xticklabels=30)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Borough',fontsize=18)\n",
    "sns_plot.vlines([30,310,409],0,1000,color='black',lw=2,ls='-.')\n",
    "sns_plot.set_yticklabels(sns_plot.get_ymajorticklabels(), fontsize = 12)\n",
    "#fig.savefig(\"heatmap.pdf\", bbox_inches='tight') \n",
    "#plt.savefig('TimeSeriesheatmap.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig5_2 = Fig5_1.T\n",
    "Fig5_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1 = plt.figure(figsize=(30,20))\n",
    "sns_plot = sns.heatmap(Fig5_2[20:80].T,cmap= \"RdBu_r\",center=-40,xticklabels=5)\n",
    "sns_plot.vlines([10,15,24],0,35,color='black',lw=2,ls='-.')\n",
    "# fig.savefig(\"heatmap.pdf\", bbox_inches='tight')\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Borough',fontsize=18)\n",
    "plt.savefig('Fig5_2.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA test\n",
    "ARIMA1 = London1['London_total']\n",
    "ARIMA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA1[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose time series to for in-depth exploration\n",
    "#decomposition = sm.tsa.seasonal_decompose(ARIMA1[:30],model='additive')\n",
    "#fig = decomposition.plot()\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ARIMA1[:25]\n",
    "valid = ARIMA1[25:30]\n",
    "treat = ARIMA1[30:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto arima test\n",
    "model = auto_arima(train,start_p=1, start_q=1,\n",
    "                         test='adf',m=7,\n",
    "                         start_P=0, seasonal=True,\n",
    "                         d=None, D=1,trace=True,\n",
    "                         error_action='ignore',  \n",
    "                         suppress_warnings=True, \n",
    "                         stepwise=True,information_criterion='aic')\n",
    "model.fit(train)\n",
    " \n",
    "forecast = model.predict(n_periods=len(valid))\n",
    "forecast = pd.DataFrame(forecast,index = ARIMA1[25:30].index,columns=['Prediction'])\n",
    " \n",
    "#plot the predictions for validation set\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(valid, label='Valid')\n",
    "plt.plot(treat,label='Treat')\n",
    "plt.plot(forecast, label='Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London_train = London1[:25]\n",
    "London_valid = London1[25:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply auto_arima to each borough\n",
    "\n",
    "ARIMA_test = pd.DataFrame(index = London_valid.index)\n",
    "\n",
    "for i in range(34):\n",
    "    train = London_train.iloc[:,i]\n",
    "    model = auto_arima(train,start_p=1, start_q=1,\n",
    "                         test='adf',m=7,\n",
    "                         start_P=0, seasonal=True,\n",
    "                         d=None, D=1,trace=True,\n",
    "                         error_action='ignore',  \n",
    "                         suppress_warnings=True, \n",
    "                         stepwise=True,information_criterion='aic')\n",
    "    model.fit(train)\n",
    "    forecast = model.predict(n_periods=10)\n",
    "    #forecast = pd.DataFrame(forecast,index = London_valid.index,columns=[London_train.columns[i]])\n",
    "    ARIMA_test[London_train.columns[i]] = forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control group fo DiD model\n",
    "\n",
    "After = ARIMA_test[5:]\n",
    "After = After.T\n",
    "After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control group fo DiD model\n",
    "\n",
    "Control = London1[0:30]\n",
    "Control = Control.T\n",
    "Control = Control.merge(After,left_index=True,right_index=True)\n",
    "Control = Control.T\n",
    "Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat group fo DiD model\n",
    "\n",
    "Treat = London1[:35]\n",
    "Treat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control group fo DiD model(test)\n",
    "\n",
    "Control1 = pd.DataFrame(Control.iloc[:,0])\n",
    "Control1['datee'] = Control1.index\n",
    "Control1['time'] = 0\n",
    "Control1['time'] = Control1['time'].where(Control1.index < '2020-03-16',1)\n",
    "Control1.index = range(0,len(Control1))\n",
    "Control1['Treat'] = 0\n",
    "Control1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat group fo DiD model(test)\n",
    "\n",
    "Treat1 = pd.DataFrame(Treat.iloc[:,0])\n",
    "Treat1['datee'] = Treat1.index\n",
    "Treat1['time'] = 0\n",
    "Treat1['time'] = Treat1['time'].where(Treat1.index < '2020-03-16',1)\n",
    "Treat1.index = range(0,len(Treat1))\n",
    "Treat1['Treat'] = 1\n",
    "Treat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DID = pd.concat([Control1,Treat1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiD model(test)\n",
    "model = 'DID.iloc[:,0] ~ Treat + time + time * Treat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula = model, data = DID)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coEff = pd.DataFrame(res.params)\n",
    "coEff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiD model to each borough \n",
    "\n",
    "coEff = pd.DataFrame()\n",
    "\n",
    "for i in range(34):\n",
    "    Control1 = pd.DataFrame(Control.iloc[:,i])\n",
    "    Control1['datee'] = Control1.index\n",
    "    Control1['time'] = 0\n",
    "    Control1['time'] = Control1['time'].where(Control1.index < '2020-03-16',1)\n",
    "    Control1.index = range(0,len(Control1))\n",
    "    Control1['Treat'] = 0\n",
    "\n",
    "    Treat1 = pd.DataFrame(Treat.iloc[:,i])\n",
    "    Treat1['datee'] = Treat1.index\n",
    "    Treat1['time'] = 0\n",
    "    Treat1['time'] = Treat1['time'].where(Treat1.index < '2020-03-16',1)\n",
    "    Treat1.index = range(0,len(Treat1))\n",
    "    Treat1['Treat'] = 1\n",
    "\n",
    "    DID = pd.concat([Control1,Treat1],axis=0)\n",
    "\n",
    "    model = 'DID.iloc[:,0] ~ Treat + time + time * Treat'\n",
    "\n",
    "    mod = smf.ols(formula = model, data = DID).fit()\n",
    "\n",
    "    coEff[Control.columns[i]] = mod.params\n",
    "\n",
    "coEff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient matrix\n",
    "\n",
    "coEff = coEff.T\n",
    "#coEff.to_csv('DID_result.csv')\n",
    "coEff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import map for visualisation\n",
    "\n",
    "path = 'https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true'\n",
    "map_bor = gpd.read_file(path)\n",
    "map_bor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DID result\n",
    "\n",
    "MobilityDecrease = pd.read_csv('DID_result.csv')\n",
    "MobilityDecrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MobilityDecrease['Borough'] = MobilityDecrease[MobilityDecrease.columns[0]]\n",
    "MobilityDecrease = MobilityDecrease.drop([MobilityDecrease.columns[0]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1 = map_bor.merge(MobilityDecrease,left_on='NAME', right_on='Borough')\n",
    "map1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,20),)\n",
    "map1.plot(column='time:Treat',ax=ax, legend=True,cmap='coolwarm')\n",
    "\n",
    "#plt.savefig('map1.png')\n",
    "#files.download(\"map1.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moran's I\n",
    "\n",
    "LocalMoran = pd.read_csv('localMoran.csv')\n",
    "maplm = map_bor.merge(LocalMoran,left_on='NAME', right_on='Borough')\n",
    "maplm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,20),)\n",
    "maplm.plot(column='Stage1',ax=ax, legend=True,cmap='RdBu_r',vmin=-1,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grocery data set\n",
    "\n",
    "url2 = 'London_GroceryPoints.csv'\n",
    "grocerypoint = pd.read_csv(url2)\n",
    "grocerypoint.rename(columns={'NAME':'Borough'}, inplace = True)\n",
    "grocerypoint.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocerypoint.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocerypoint.groupby('retailer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocerypoint['size_band'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocerypoint.groupby('size_band').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceryCom = pd.DataFrame(grocerypoint.groupby(['size_band','Borough']).size())\n",
    "groceryCom = groceryCom.unstack(level=0,fill_value=0)\n",
    "#groceryCom = groceryCom.T\n",
    "\n",
    "#groceryCom.index = range(0,len(groceryCom))\n",
    "groceryCom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceryCom1 = groceryCom.div(groceryCom.sum(axis=1), axis=0)\n",
    "groceryCom1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journeytime data set\n",
    "\n",
    "Journeytime = pd.read_csv('JourneyTime.csv')\n",
    "Journeytime = Journeytime.drop(['LSOA_code','Region','LA_Code'],axis=1)\n",
    "Journeytime = Journeytime.groupby(['LA_Name']).mean()\n",
    "\n",
    "Journeytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population by age group\n",
    "\n",
    "AgeGroup = pd.read_csv('populationByAge.csv')\n",
    "AgeGroup = AgeGroup.drop(['Code',' ALL AGES'],axis=1)\n",
    "#Journeytime = Journeytime.groupby(['LA_Name']).mean()\n",
    "\n",
    "AgeGroup.index = AgeGroup['Area']\n",
    "AgeGroup = AgeGroup.drop(['Area'],axis=1)\n",
    "AgeGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgeGroup1 = AgeGroup.div(AgeGroup.sum(axis=1), axis=0)\n",
    "AgeGroup1['Total_pop'] = AgeGroup.sum(axis=1)\n",
    "AgeGroup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points within a distance\n",
    "\n",
    "NumPoints = pd.read_csv('NumPoints_Distance_byBorough.csv')\n",
    "NumPoints.index = NumPoints['NAME']\n",
    "NumPoints = NumPoints.drop(['GSS_CODE','NAME'],axis=1)\n",
    "NumPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income deprivation\n",
    "\n",
    "IMD = pd.read_csv('IMD.csv')\n",
    "IMD.index = IMD['Borough']\n",
    "IMD = IMD.drop(['Borough'],axis=1)\n",
    "\n",
    "IMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car ownership\n",
    "\n",
    "CarOwnership = pd.read_csv('CarOwnership.csv')\n",
    "CarOwnership.index = CarOwnership['Borough']\n",
    "CarOwnership = CarOwnership.drop(['Borough'],axis=1)\n",
    "\n",
    "CarOwnership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CarOwnership1 = CarOwnership.drop(['Cars: sum of All cars or vans in the area; measures: Value'],axis=1)\n",
    "CarOwnership1 = CarOwnership1.div(CarOwnership1.sum(axis=1), axis=0)\n",
    "CarOwnership1['Total_car'] = CarOwnership['Cars: sum of All cars or vans in the area; measures: Value']\n",
    "CarOwnership1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to grocery store\n",
    "\n",
    "DistanceToGrocery = pd.read_csv('LSOA_grocerypoints.csv')\n",
    "g = DistanceToGrocery.groupby(['Borough'])\n",
    "newdf = g.apply(lambda x: pd.Series([np.average(x['HubDist'], weights=x['All Ages']), \n",
    "                             np.average(x['NUMPOINTS('], weights=x['All Ages']),np.average(x['NUMPOINT_1'], weights=x['All Ages']),np.average(x['NUMPOINT_2'], weights=x['All Ages']),], \n",
    "                                    index=['WA_HubDist','WA_NUMPOINTS(500m)','WA_NUMPOINTS(1km)','WA_NUMPOINTS(2km)']))\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2 = map1.merge(groceryCom1,left_on='NAME', right_on= groceryCom1.index)\n",
    "map2 = map2.merge(Journeytime,left_on='NAME', right_on= Journeytime.index)\n",
    "map2 = map2.merge(NumPoints,left_on='NAME', right_on= NumPoints.index)\n",
    "map2 =  map2.merge(AgeGroup1,left_on='NAME', right_on= AgeGroup1.index)\n",
    "map2 =  map2.merge(IMD,left_on='NAME', right_on= IMD.index)\n",
    "map2 =  map2.merge(CarOwnership1,left_on='NAME', right_on= CarOwnership1.index)\n",
    "map2 = map2.merge(newdf,left_on='NAME', right_on= newdf.index)\n",
    "\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population density\n",
    "\n",
    "map2['pop_den'] = map2['Total_pop']/map2['HECTARES']\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbour Index\n",
    "\n",
    "NNI = pd.read_csv('NNIresult.csv')\n",
    "map2 = map2.merge(NNI,left_on='NAME', right_on= 'Borough')\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workplace population\n",
    "\n",
    "workplace = pd.read_csv('workplace.csv')\n",
    "workplace.index = workplace['Borough']\n",
    "workplace = workplace.drop(['Borough'],axis=1)\n",
    "workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2 = map2.merge(workplace,left_on='NAME', right_on= workplace.index)\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2['employee/pop'] = map2['Employee']/map2['Total_pop']\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correff = map2.drop(['GSS_CODE','HECTARES','NONLD_AREA','ONS_INNER','geometry','Intercept','Treat','time','Borough_x','Borough_y'],axis=1)\n",
    "correff.index = correff['NAME']\n",
    "correff = correff.drop(['NAME','FoodPT45n','FoodPT60n','FoodPT45pct','FoodPT60pct','FoodCar30n','FoodCar45n',\n",
    "                        'FoodCar60n','FoodCar15pct','FoodCar30pct','FoodCar45pct','FoodCar60pct'],axis=1)\n",
    "correff\n",
    "#correff = correff.drop(['City of London','Tower Hamlets'],axis=0)\n",
    "correff.to_csv('correff1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrStage1 = pd.read_csv('correffStage1.csv')\n",
    "corrStage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis\n",
    "\n",
    "corrMatrix = correff.corr()\n",
    "#f, ax = plt.subplots(figsize=(50, 50))\n",
    "#sns.heatmap(corrMatrix, vmax=.8, square=True,annot=True,cmap='BrBG')\n",
    "\n",
    "#plt.savefig('heatmap.png', dpi=300)\n",
    "#plt.show()\n",
    "corrMatrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix\n",
    "corrMatrix.to_csv('corrMatrixStage1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correfftest = pd.read_csv('correff1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(correfftest.drop(['time:Treat','NAME'], axis=1), correfftest['time:Treat'], test_size = 0.10, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Model Test 1 \n",
    "# increase the max_iter to guarantee convergence\n",
    "lasso_model = linear_model.Lasso(max_iter=10e7, normalize=True)\n",
    "lasso_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Model Test 2\n",
    "reg_tree = DecisionTreeRegressor(random_state=0)\n",
    "reg_tree.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree.score(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression Model\n",
    "reg_random_forest = RandomForestRegressor(random_state=0,oob_score=True)\n",
    "reg_random_forest.fit(X_train, y_train.values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#Grid_List = {'max_depth':[int(x) for x in np.linspace(10, 100, num = 10)],'n_estimators':[int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]}\n",
    "#grid_search = GridSearchCV(estimator=reg_random_forest,cv=5,param_grid=Grid_List)\n",
    "#grid_search.fit(X=X_train, y=y_train.values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CVResults = pd.DataFrame(grid_search.cv_results_)\n",
    "#CVResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_random_forest.score(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_random_forest.score(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg_random_forest.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_random_forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grocery data set(LSOA)\n",
    "\n",
    "urlLSOA = 'GrocerybyLSOA.csv'\n",
    "grocerypointLSOA = pd.read_csv(urlLSOA)\n",
    "#grocerypointLSOA.rename(columns={'NAME':'Borough'}, inplace = True)\n",
    "grocerypointLSOA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceryComLSOA = pd.DataFrame(grocerypointLSOA.groupby(['size_band','LSOA11CD']).size())\n",
    "groceryComLSOA = groceryComLSOA.unstack(level=0,fill_value=0)\n",
    "#groceryCom = groceryCom.T\n",
    "\n",
    "#groceryCom.index = range(0,len(groceryCom))\n",
    "groceryComLSOA = groceryComLSOA.div(groceryComLSOA.sum(axis=1), axis=0)\n",
    "groceryComLSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population by age group LSOA\n",
    "\n",
    "AgeGroupLSOA = pd.read_csv('LSOA_population.csv')\n",
    "AgeGroupLSOA.index = AgeGroupLSOA['LSOA Code']\n",
    "AgeGroupLSOA = AgeGroupLSOA.drop(['LSOA Code'],axis=1)\n",
    "AgeGroupLSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgeGroupLSOA1 = AgeGroupLSOA.div(AgeGroupLSOA.sum(axis=1), axis=0)\n",
    "AgeGroupLSOA1['Total_pop'] = AgeGroupLSOA.sum(axis=1)\n",
    "AgeGroupLSOA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points at LSOA level\n",
    "\n",
    "NumPointsLSOA = pd.read_csv('LSOA_NumPoints.csv')\n",
    "NumPointsLSOA.index = NumPointsLSOA['LSOA11 Code']\n",
    "NumPointsLSOA = NumPointsLSOA.drop(['LSOA11 Code'],axis=1)\n",
    "NumPointsLSOA = NumPointsLSOA.fillna(0)\n",
    "NumPointsLSOA.rename(columns={'id_count':'NumPoint','NUMPOINTS(':'NUMPOINTS(500m)','NUMPOINT_1':'NUMPOINTS(1km)','NUMPOINT_2':'NUMPOINTS(2km)'}, inplace = True)\n",
    "NumPointsLSOA['Area'] = 10*NumPointsLSOA['Area']\n",
    "NumPointsLSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car ownership LSOA level\n",
    "\n",
    "CarOwnershipLSOA = pd.read_csv('LSOA_carownership1.csv')\n",
    "CarOwnershipLSOA.index = CarOwnershipLSOA['lsoa11cd']\n",
    "CarOwnershipLSOA = CarOwnershipLSOA.drop(['lsoa11cd'],axis=1)\n",
    "\n",
    "CarOwnershipLSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CarOwnershipLSOA1 = CarOwnershipLSOA.drop(['Cars: sum of All cars or vans in the area; measures: Value'],axis=1)\n",
    "CarOwnershipLSOA1 = CarOwnershipLSOA1.div(CarOwnershipLSOA1.sum(axis=1), axis=0)\n",
    "CarOwnershipLSOA1['Total_car'] = CarOwnershipLSOA['Cars: sum of All cars or vans in the area; measures: Value']\n",
    "CarOwnershipLSOA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journey time LSOA level\n",
    "\n",
    "LSOAJourneytime = pd.read_csv('LSOA_JourneyTime.csv')\n",
    "LSOAJourneytime.index = LSOAJourneytime['LSOA_code']\n",
    "LSOAJourneytime = LSOAJourneytime.drop(['LSOA_code'],axis=1)\n",
    "LSOAJourneytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workplace population LSOA level\n",
    "\n",
    "workplaceLSOA = pd.read_csv('LSOA_Employee1.csv')\n",
    "workplaceLSOA.index = workplaceLSOA['LSOA11CD']\n",
    "workplaceLSOA = workplaceLSOA.drop(['LSOA11CD'],axis=1)\n",
    "workplaceLSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'LSOA_London.gpkg'\n",
    "map_lsoa = gpd.read_file(path1)\n",
    "map_lsoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSOA_variables = pd.DataFrame(map_lsoa['LSOA11CD'])\n",
    "LSOA_variables = LSOA_variables.merge(groceryComLSOA,right_on=groceryComLSOA.index, left_on='LSOA11CD',how='left').fillna(0)\n",
    "LSOA_variables = LSOA_variables.merge(LSOAJourneytime,right_on=LSOAJourneytime.index, left_on='LSOA11CD')\n",
    "LSOA_variables = LSOA_variables.merge(NumPointsLSOA,right_on=NumPointsLSOA.index, left_on='LSOA11CD')\n",
    "LSOA_variables = LSOA_variables.merge(AgeGroupLSOA1,right_on=AgeGroupLSOA1.index, left_on='LSOA11CD')\n",
    "LSOA_variables = LSOA_variables.merge(CarOwnershipLSOA1,right_on=CarOwnershipLSOA1.index, left_on='LSOA11CD')\n",
    "LSOA_variables = LSOA_variables.merge(workplaceLSOA,right_on=workplaceLSOA.index, left_on='LSOA11CD')\n",
    "\n",
    "LSOA_variables['pop_den'] = LSOA_variables['Total_pop']/LSOA_variables['Area']\n",
    "LSOA_variables['employee/pop'] = LSOA_variables['Employee']/LSOA_variables['Total_pop']\n",
    "LSOA_variables.index = LSOA_variables['LSOA11CD']\n",
    "LSOA_variables = LSOA_variables.drop(['LSOA11CD'],axis=1)\n",
    "\n",
    "#LSOA_variables.to_csv('LSOA_variables.csv')\n",
    "\n",
    "LSOA_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSOA_variables1 = pd.read_csv('LSOA_variables_p.csv')\n",
    "LSOA_variables1.index = LSOA_variables1['LSOA11CD']\n",
    "LSOA_variables1 = LSOA_variables1.drop(['LSOA11CD'],axis=1)\n",
    "LSOA_variables1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "lsoa_pred = pd.DataFrame(reg_random_forest.predict(LSOA_variables1))\n",
    "lsoa_pred.index = LSOA_variables1.index\n",
    "#soa_pred.to_csv('Stage1pre.csv')\n",
    "\n",
    "lsoa_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stage1 = pd.read_csv('Stage1pre.csv')\n",
    "Stage1.index = Stage1['LSOA11CD']\n",
    "Stage1 = Stage1.drop(['LSOA11CD'],axis=1)\n",
    "Stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stage2 = pd.read_csv('Stage2pre.csv')\n",
    "Stage2.index = Stage2['LSOA11CD']\n",
    "Stage2 = Stage2.drop(['LSOA11CD'],axis=1)\n",
    "Stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stage3 = pd.read_csv('Stage3pre.csv')\n",
    "Stage3.index = Stage3['LSOA11CD']\n",
    "Stage3 = Stage3.drop(['LSOA11CD'],axis=1)\n",
    "Stage3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Offline = Stage1.merge(Stage2,left_on=Stage1.index, right_on=Stage2.index)\n",
    "Offline = Offline.merge(Stage3,left_on='key_0', right_on=Stage3.index)\n",
    "Offline.index = Offline['key_0']\n",
    "Offline = Offline.drop(['key_0'],axis=1)\n",
    "Offline\n",
    "Offline.to_csv('PreLSOA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data standardization\n",
    "rs = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "\n",
    "for c in Offline.columns.values:\n",
    "    Offline[c] = rs.fit_transform(Offline[c].values.reshape(-1,1))\n",
    "    \n",
    "Offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "\n",
    "Online = pd.read_csv('iuc.csv')\n",
    "Online.index = Online['LSOA11_CD']\n",
    "Online = Online.drop(['LSOA11_CD'],axis=1)\n",
    "Online1 = pd.get_dummies(Online['OnlineGroup'],prefix='OnlineGroup')\n",
    "Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indicators = Offline.merge(Online1,left_on= Offline.index, right_on=Online.index,how='left')\n",
    "Indicators.index = Indicators['key_0']\n",
    "Indicators = Indicators.drop(['key_0'],axis=1)\n",
    "\n",
    "Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means Clustering\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for k in range(2,41):\n",
    "    \n",
    "    #############\n",
    "    # Do the clustering using the main columns\n",
    "    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(Indicators)\n",
    "    \n",
    "    # Calculate the overall silhouette score\n",
    "    silhouette_avg = silhouette_score(Indicators, kmeans.labels_)\n",
    "    \n",
    "    y.append(k)\n",
    "    x.append(silhouette_avg)\n",
    "    \n",
    "    print('', end='')\n",
    "\n",
    "print()\n",
    "print(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.plot(y, x)\n",
    "plt.ylabel('Average Silhouette Scores',fontsize=18)\n",
    "plt.xlabel('Number of clusters',fontsize=18)\n",
    "plt.savefig('Fig5N.png') \n",
    "#plt.gcf().suptitle(\"Average Silhouette Scores\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate SSE for a range of number of cluster\n",
    "list_SSE = []\n",
    "min_k = 1\n",
    "max_k = 10\n",
    "range_k = range(min_k, max_k+1)\n",
    "for i in range_k:\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=300,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(Indicators)\n",
    "    # inertia is a concept in physics. Roughly it means SSE of clustering.\n",
    "    list_SSE.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(range_k, list_SSE, marker='o')\n",
    "plt.ylabel('SSE',fontsize=18)\n",
    "plt.xlabel('Number of clusters',fontsize=18)\n",
    "plt.savefig('Fig5N1.png') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_pref=7\n",
    "    \n",
    "#############\n",
    "# Do the clustering using the main columns\n",
    "kmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(Indicators)\n",
    "\n",
    "# Convert to a series\n",
    "s = pd.Series(kmeans.labels_, index=Indicators.index, name='K-means')\n",
    "\n",
    "# We do this for plotting\n",
    "Indicators1 = Indicators\n",
    "Indicators1['K-means'] = s\n",
    "    \n",
    "# Calculate the overall silhouette score\n",
    "silhouette_avg = silhouette_score(Indicators, kmeans.labels_)\n",
    "\n",
    "# Calculate the silhouette values\n",
    "sample_silhouette_values = silhouette_samples(Indicators, kmeans.labels_)\n",
    "    \n",
    "#############\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(9, 5)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1\n",
    "ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n",
    "    \n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, Indicators1.shape[0] + (k + 1) * 10])\n",
    "    \n",
    "y_lower = 10\n",
    "    \n",
    "# For each of the clusters...\n",
    "for i in range(k_pref):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[kmeans.labels_ == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "    # Set the color ramp\n",
    "    color = plt.cm.Spectral(i/k)\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, ith_cluster_silhouette_values,\n",
    "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed --\n",
    "    # we can only do this for the first two dimensions\n",
    "    # so we may not see fully what is causing the \n",
    "    # resulting assignment\n",
    "    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n",
    "    ax2.scatter(Indicators1[Indicators1.columns[0]], Indicators1[Indicators1.columns[1]], \n",
    "                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"Visualization of the clustered data\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette results for KMeans clustering \"\n",
    "                \"with %d clusters\" % k_pref),\n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indicators['K-means']=kmeans.labels_\n",
    "#Indicators.to_csv('KmeansResult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = None\n",
    "for k in sorted(Indicators1['K-means'].unique()):\n",
    "    print(f\"Processing cluster {k}\")\n",
    "\n",
    "    c = Indicators1[Indicators1['K-means']==k]\n",
    "    if centroids is None:\n",
    "        centroids = pd.DataFrame(columns=c.columns.values)\n",
    "    centroids = centroids.append(c.mean(), ignore_index=True)\n",
    "    \n",
    "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
    "for i in range(0,len(centroids.index)):\n",
    "    row = centroids.iloc[i,:]\n",
    "    c_index = list(centroids.columns.values).index('K-means')\n",
    "    for c in range(0,c_index):\n",
    "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
    "        odf = odf.append(d, ignore_index=True)\n",
    "        \n",
    "odf = odf[~odf.Variable.isin(['Borough','msoa11hclnm','Subregion','geometry'])]\n",
    "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
    "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary = Indicators.merge(Online,left_on= Indicators.index, right_on=Online.index,how='left')\n",
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary.to_csv('KmeansResult2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVL = pd.read_csv('LastDD.csv')\n",
    "SVL.index = SVL['LSOA']\n",
    "SVL = SVL.drop('LSOA',axis=1)\n",
    "SVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = SVL.groupby('K-means').mean()\n",
    "#c.to_csv('clusterResultLL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = SVL.groupby('K-means')['OnlineGroup'].nunique()\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = SVL.groupby('K-means').count()\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(7,1,figsize=(18,18))\n",
    "for k in range(7):\n",
    "\n",
    "\n",
    "    anne = SVL[SVL['K-means']==k]\n",
    "    anne = anne['Stage3'].tolist()\n",
    "    axs[k].hist(anne)\n",
    "    n = k+1\n",
    "    axs[k].set_ylabel('Cluster'\" %d\" % n)\n",
    "plt.savefig('Fig5.18.png') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
